#!/bin/bash
set -euo pipefail

if ! hash nextstrain 2>/dev/null; then
    echo "The Nextstrain CLI must be installed." >&2
    exit 1
fi

: "${SLACK_TOKEN:?The SLACK_TOKEN environment variable is required.}"

bin="$(dirname "$0")"

main() {
    if [[ $# -ne 0 ]]; then
        local metadata_src="${1:?A source metadata TSV file is required as the first argument.}"
        local metadata_dst="${2:?A destination metadata TSV s3:// URL is required as the second argument.}"

        local sequences_src="${3:?A source sequences fasta file is required as the third argument.}"
        local sequences_dst="${4:?A destination sequences fasta s3:// URL is required as the fourth argument.}"

        if hashes-match "$metadata_src" "$metadata_dst" &&
           hashes-match "$sequences_src" "$sequences_dst"
        then
            echo "No metadata or sequence changes"
            echo "Exiting"
            exit 0
        fi

        echo "Changes detected in metadata or sequences. Continuing"
    else
        echo "Rebuilding using remote data files."
    fi

    if [[ -d "ncov" ]]; then
        echo "Downloading latest version of the ncov repo (master branch)"
        (cd ncov; git pull)
    else
        echo "Cloning the ncov repo"
        git clone https://github.com/czbiohub/ncov.git
    fi

    if [[ -n "${metadata_src:-}" && -n "${sequences_src:-}" ]]; then
        # Run on these exact data files by uploading them with the build context for
        # the Batch job, instead of roundtripping them through S3.
        #
        # Since the Batch job is submitted before the rest of the GitHub workflow
        # updates the files on S3, this also avoids a narrow race condition if AWS
        # Batch is quick or the upload to S3 slow.
        mkdir -p ncov/data/
        cp -v "$metadata_src" "$sequences_src" ncov/data/
    fi

    cp "$bin"/../rules/czbiohub.smk ncov

    local output=$(
        "$bin"/launch-build \
            ncov upload_all \
            --config slack_token="$SLACK_TOKEN" slack_channel="$SLACK_CHANNELS" s3_staging_url="$S3_DST"/results localrules=czbiohub.smk
    )

    echo "$output"

    # Extract the AWS job ID from the `nextstrain build --aws-batch --detach` output
    local aws_batch_job_id=$(grep "AWS Batch Job ID" <<<"$output" | cut -d ' ' -f 5)

    echo "Notifying Slack about rebuild."
    "$bin"/notify-slack "A new staging build was submitted. Follow along in your local ncov repo with: "'```'"nextstrain build --aws-batch --attach $aws_batch_job_id ."'```'
}

# Returns 1 if both files match (have identical hashes). Else returns 0.
hashes-match() {
    local src_hash="$("$bin/sha256sum" < "$1")"

    local dst_s3path="${2#s3://}"
    local dst_bucket="${dst_s3path%%/*}"
    local dst_key="${dst_s3path#*/}"
    local dst_hash="$(aws s3api head-object --bucket "$dst_bucket" --key "$dst_key" --query Metadata.sha256sum --output text)"

    [[ "$src_hash" == "$dst_hash" ]]
}

main "$@"
